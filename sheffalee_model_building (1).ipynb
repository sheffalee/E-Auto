{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cVWDN-1Mz1ll"
      },
      "source": [
        "Task 3 : Model Building\n",
        "\n",
        "\n",
        "Below discussed are the popular and effective models for car brand identification:\n",
        "\n",
        "1.Convolutional Neural Networks (CNNs):\n",
        "\n",
        "CNNs have been the go-to choice for image classification tasks, including car brand identification. You can use architectures like VGG, ResNet, or MobileNet as a base and fine-tune them for your specific dataset.\n",
        "\n",
        "2.Transfer Learning with Inception or ResNet:\n",
        "\n",
        "Leveraging pre-trained Inception or ResNet models and fine-tuning them on your car brand dataset can often yield excellent results. Transfer learning helps save time and resources.\n",
        "\n",
        "3.Support Vector Machines (SVM):\n",
        "\n",
        "SVMs are powerful classifiers that can work well with appropriate feature extraction methods like HOG or SIFT. They are known for their ability to handle complex decision boundaries.\n",
        "\n",
        "4.Random Forest:\n",
        "\n",
        "Random Forest is an ensemble learning technique that can handle high-dimensional data well. It's robust and can be effective for car brand identification, especially when combined with appropriate feature engineering.\n",
        "\n",
        "5.Bag-of-Words (BoW) with K-Nearest Neighbors (KNN):\n",
        "\n",
        "This combination of BoW for feature extraction and KNN for classification can work surprisingly well for car brand identification. It's simple and interpretable, making it a good choice for some scenarios.\n",
        "\n",
        "In this model building, i have used transfer learning models. Below is the detailed explanation of the transfer learning models:\n",
        "\n",
        "Transfer learning is a machine learning technique where a model trained on one task is adapted for use on a second, related task. In the context of car brand identification or any image classification task, transfer learning has become a popular and effective approach. Here's an overview of transfer learning models for image classification:\n",
        "\n",
        "**1. Pre-trained Convolutional Neural Networks (CNNs):**\n",
        "   - CNNs have revolutionized image classification tasks. Pre-trained CNN models, such as VGG, ResNet, Inception, and MobileNet, have been trained on massive image datasets like ImageNet, which contain a vast variety of objects, including cars. These models have learned to extract hierarchical and generic features from images, making them excellent feature extractors.\n",
        "\n",
        "   - Transfer learning using pre-trained CNNs involves taking the layers of a pre-trained network and fine-tuning them for a specific task, such as car brand identification. The early layers capture low-level features like edges and textures, while deeper layers capture more abstract and high-level features. You can retrain the last few layers or some intermediate layers to adapt the model to your dataset.\n",
        "\n",
        "   - Benefits of using pre-trained CNNs include reduced training time and the ability to leverage the rich features learned from a large and diverse dataset like ImageNet.\n",
        "\n",
        "**2. Feature Extraction with CNNs:**\n",
        "   - Another transfer learning approach is to use a pre-trained CNN as a fixed feature extractor. You can remove the top classification layers and use the output of the last convolutional layer as feature vectors. These feature vectors can then be fed into a separate classifier (e.g., SVM, Random Forest) for car brand identification.\n",
        "\n",
        "   - This approach is particularly useful when you have limited labeled data for your specific task. It allows you to benefit from the generalization capabilities of pre-trained CNNs while training a simple classifier on top of the extracted features.\n",
        "\n",
        "**3. Fine-tuning Pre-trained Models:**\n",
        "   - Fine-tuning involves training the entire pre-trained model on your target dataset. You start with the pre-trained weights and then update them during training on your dataset. This can be beneficial when your target task is similar to the original task the model was trained on.\n",
        "\n",
        "   - For car brand identification, you can fine-tune a pre-trained model by replacing the output layer with a new set of output units corresponding to the car brands in your dataset. The rest of the model's layers can be updated during training.\n",
        "\n",
        "**4. Domain Adaptation:**\n",
        "   - If your target domain (e.g., images of cars in real-world conditions) is substantially different from the source domain (e.g., ImageNet), domain adaptation techniques can be applied. These techniques aim to reduce the domain gap between the source and target data to improve model performance.\n",
        "\n",
        "Transfer learning with pre-trained models has proven to be highly effective in various image classification tasks, including car brand identification. It allows you to leverage the knowledge encoded in large, publicly available datasets and adapt it to your specific problem, often resulting in better performance with less data and computation compared to training from scratch. However, selecting the right pre-trained model and fine-tuning strategy depends on the nature of your dataset and the specific requirements of your task.\n",
        "\n",
        "The transfer learning model i chose is VGG.Below is the detailed explanation of the VGG model:\n",
        "\n",
        "The VGG (Visual Geometry Group) model is a deep convolutional neural network architecture designed for image classification. It was developed by the Visual Geometry Group at the University of Oxford and was one of the finalists in the ImageNet Large Scale Visual Recognition Challenge (ILSVRC) in 2014. The VGG architecture is known for its simplicity and effectiveness, making it a popular choice for image classification tasks.\n",
        "\n",
        "Here is a detailed explanation of the VGG model:\n",
        "\n",
        "**1. Architecture:**\n",
        "   - The VGG model consists of multiple layers, primarily using 3x3 convolutional filters, with max-pooling layers interspersed. The key idea behind VGG is to use very small 3x3 convolutional filters repeatedly, which allows the network to learn complex features while keeping the number of parameters manageable.\n",
        "\n",
        "**2. Layer Configurations:**\n",
        "   - There are several variations of the VGG architecture, denoted as VGG11, VGG13, VGG16, and VGG19, depending on the depth of the network. The numbers in the names represent the total number of weight layers (including both convolutional and fully connected layers).\n",
        "\n",
        "   - For example, VGG16 consists of 16 weight layers: 13 convolutional layers followed by 3 fully connected layers.\n",
        "\n",
        "**3. Convolutional Blocks:**\n",
        "   - Each convolutional block in VGG typically consists of two or more 3x3 convolutional layers followed by a max-pooling layer. The convolutional layers are usually followed by Rectified Linear Unit (ReLU) activation functions to introduce non-linearity.\n",
        "\n",
        "   - After a series of convolutional blocks, the spatial dimensions of the feature maps decrease, while the depth increases.\n",
        "\n",
        "**4. Max-Pooling:**\n",
        "   - Max-pooling layers are used to reduce the spatial dimensions of the feature maps while retaining the most important information. In VGG, max-pooling is typically performed using 2x2 windows with a stride of 2.\n",
        "\n",
        "**5. Fully Connected Layers:**\n",
        "   - After the convolutional layers, VGG includes fully connected layers for final classification. These fully connected layers are followed by softmax activation to produce class probabilities.\n",
        "\n",
        "**6. Number of Parameters:**\n",
        "   - One of the characteristics of VGG is its simplicity in terms of architectural design. However, this simplicity comes at the cost of a large number of parameters, especially in deeper versions like VGG16 and VGG19. This can make training and deploying these models computationally expensive.\n",
        "\n",
        "**7. Pre-trained Models:**\n",
        "   - Due to its effectiveness, VGG models pre-trained on large image datasets like ImageNet are often used as the starting point for various computer vision tasks. Researchers and practitioners fine-tune these pre-trained models on specific datasets for tasks like object detection, image segmentation, and car brand identification.\n",
        "\n",
        "**8. Limitations:**\n",
        "   - While VGG was groundbreaking when it was introduced, newer architectures like ResNet and Inception have since surpassed it in terms of accuracy and efficiency. VGG's depth and parameter count can make it less suitable for real-time applications or resource-constrained environments.\n",
        "\n",
        "In summary, the VGG model is a deep convolutional neural network architecture known for its simplicity and effectiveness in image classification tasks. It uses small 3x3 convolutional filters repeatedly to learn features from images and has different variations depending on the depth of the network. Pre-trained VGG models have been widely used as a starting point for various computer vision tasks, making them valuable tools in the field of deep learning.\n",
        "\n",
        "\n",
        "Dataset Link:https://drive.google.com/drive/folders/18zhtVKfTju_rz7XnGe_QRpASijOdBqBh?usp=sharing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yv8P_9xOIlKE"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gjVYAClvIuMX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "899605f9-7f60-41ff-9fee-e198237d6cfc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4_lX7yehIuPQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e68b52e3-3160-42ee-9ec5-a073bc0e9326"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['file1.pdf',\n",
              " 'WhatsApp Image 2023-01-16 at 12.55.13 (1).jpeg',\n",
              " 'WhatsApp Image 2023-01-16 at 12.55.13 (2).jpeg',\n",
              " 'cir_new.pdf',\n",
              " 'WhatsApp Image 2023-01-16 at 12.55.13.jpeg',\n",
              " 'WhatsApp Image 2023-01-16 at 12.55.13 (3).jpeg',\n",
              " 'DAESI_interview.pdf',\n",
              " 'resume_siva_compressed.pdf',\n",
              " 'Document from Kumar Singh Sharma (1).pdf',\n",
              " 'Document from Kumar Singh Sharma (1)',\n",
              " 'Photo from Kumar Singh Sharma (1)',\n",
              " 'Photo from Kumar Singh Sharma',\n",
              " 'Document from Kumar Singh Sharma.pdf',\n",
              " 'Document from Kumar Singh Sharma',\n",
              " 'kimaqr bolld (1).docx',\n",
              " 'WhatsApp Image 2023-07-16 at 22.10.38.jpeg',\n",
              " 'CERTIFICATES (1)-1.pdf',\n",
              " 'WhatsApp Image 2023-07-16 at 22.10.38 (1).jpeg',\n",
              " 'sraco.jpeg',\n",
              " 'preprocessed_dataset.zip',\n",
              " 'my_dataset',\n",
              " 'siva.gslides',\n",
              " 'sivakumar_resume.pdf',\n",
              " 'sivakumar_resume.gdoc',\n",
              " 'Colab Notebooks',\n",
              " 'datasets']"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "drive_root = '/content/drive/My Drive'\n",
        "os.listdir(drive_root)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wdy1TrH2IuRj"
      },
      "outputs": [],
      "source": [
        "from keras.applications import VGG16"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-xXylBJpIuUS"
      },
      "outputs": [],
      "source": [
        "img_rows, img_cols = 224, 224"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "admI1A7kIuXY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "387acf5c-758c-4909-b7dc-d59329fc3f3a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "58889256/58889256 [==============================] - 4s 0us/step\n"
          ]
        }
      ],
      "source": [
        "model = VGG16(weights = 'imagenet',\n",
        "                 include_top = False,\n",
        "                 input_shape = (img_rows, img_cols, 3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IuEF1mX5IuaV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "60a7c612-7a2e-47d2-9bd6-a18878db455d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 InputLayer False\n",
            "1 Conv2D False\n",
            "2 Conv2D False\n",
            "3 MaxPooling2D False\n",
            "4 Conv2D False\n",
            "5 Conv2D False\n",
            "6 MaxPooling2D False\n",
            "7 Conv2D False\n",
            "8 Conv2D False\n",
            "9 Conv2D False\n",
            "10 MaxPooling2D False\n",
            "11 Conv2D False\n",
            "12 Conv2D False\n",
            "13 Conv2D False\n",
            "14 MaxPooling2D False\n",
            "15 Conv2D False\n",
            "16 Conv2D False\n",
            "17 Conv2D False\n",
            "18 MaxPooling2D False\n"
          ]
        }
      ],
      "source": [
        "# Layers are set to trainable as True by default\n",
        "for layer in model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Let's print our layers\n",
        "for (i,layer) in enumerate(model.layers):\n",
        "    print(str(i) + \" \"+ layer.__class__.__name__, layer.trainable)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8IecRgjzIufA"
      },
      "outputs": [],
      "source": [
        "\n",
        "def add_layer(bottom_model, num_classes):\n",
        "    \"\"\"creates the top or head of the model that will be\n",
        "    placed ontop of the bottom layers\"\"\"\n",
        "\n",
        "    top_model = bottom_model.output\n",
        "    top_model = GlobalAveragePooling2D()(top_model)\n",
        "    top_model = Dense(1024,activation='relu')(top_model)\n",
        "    top_model = Dense(512,activation='relu')(top_model)\n",
        "    top_model = Dense(num_classes,activation='softmax')(top_model)\n",
        "    return top_model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y7ontMgAIuh0"
      },
      "outputs": [],
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten, GlobalAveragePooling2D\n",
        "from keras.layers import Conv2D, MaxPooling2D, ZeroPadding2D\n",
        "from keras.models import Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FP0uvmH2Iukz"
      },
      "outputs": [],
      "source": [
        "num_classes = 9"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dFk4Y8qMIund",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "67304a15-09a2-4386-8ed3-b46809bea386"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
            "                                                                 \n",
            " block1_conv1 (Conv2D)       (None, 224, 224, 64)      1792      \n",
            "                                                                 \n",
            " block1_conv2 (Conv2D)       (None, 224, 224, 64)      36928     \n",
            "                                                                 \n",
            " block1_pool (MaxPooling2D)  (None, 112, 112, 64)      0         \n",
            "                                                                 \n",
            " block2_conv1 (Conv2D)       (None, 112, 112, 128)     73856     \n",
            "                                                                 \n",
            " block2_conv2 (Conv2D)       (None, 112, 112, 128)     147584    \n",
            "                                                                 \n",
            " block2_pool (MaxPooling2D)  (None, 56, 56, 128)       0         \n",
            "                                                                 \n",
            " block3_conv1 (Conv2D)       (None, 56, 56, 256)       295168    \n",
            "                                                                 \n",
            " block3_conv2 (Conv2D)       (None, 56, 56, 256)       590080    \n",
            "                                                                 \n",
            " block3_conv3 (Conv2D)       (None, 56, 56, 256)       590080    \n",
            "                                                                 \n",
            " block3_pool (MaxPooling2D)  (None, 28, 28, 256)       0         \n",
            "                                                                 \n",
            " block4_conv1 (Conv2D)       (None, 28, 28, 512)       1180160   \n",
            "                                                                 \n",
            " block4_conv2 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
            "                                                                 \n",
            " block4_conv3 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
            "                                                                 \n",
            " block4_pool (MaxPooling2D)  (None, 14, 14, 512)       0         \n",
            "                                                                 \n",
            " block5_conv1 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
            "                                                                 \n",
            " block5_conv2 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
            "                                                                 \n",
            " block5_conv3 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
            "                                                                 \n",
            " block5_pool (MaxPooling2D)  (None, 7, 7, 512)         0         \n",
            "                                                                 \n",
            " global_average_pooling2d (  (None, 512)               0         \n",
            " GlobalAveragePooling2D)                                         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1024)              525312    \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 512)               524800    \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 9)                 4617      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 15769417 (60.16 MB)\n",
            "Trainable params: 1054729 (4.02 MB)\n",
            "Non-trainable params: 14714688 (56.13 MB)\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "FC_Head = add_layer(model, num_classes)\n",
        "\n",
        "modelnew = Model(inputs = model.input, outputs = FC_Head)\n",
        "\n",
        "print(modelnew.summary())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vNfYzs1jIuqi"
      },
      "outputs": [],
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s3XmELmsIutN"
      },
      "outputs": [],
      "source": [
        "train_data_dir = '/content/drive/MyDrive/datasets/train'\n",
        "validation_data_dir = '/content/drive/MyDrive/datasets/validation'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sbQUdN7XIuv8"
      },
      "outputs": [],
      "source": [
        "# Let's use some data augmentaiton\n",
        "train_datagen = ImageDataGenerator(\n",
        "      rescale=1./255,\n",
        "      rotation_range=45,\n",
        "      width_shift_range=0.3,\n",
        "      height_shift_range=0.3,\n",
        "      horizontal_flip=True,\n",
        "      fill_mode='nearest')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xyKGxDklIuzG"
      },
      "outputs": [],
      "source": [
        "validation_datagen = ImageDataGenerator(rescale=1./255)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vJHoqIEPIu2C"
      },
      "outputs": [],
      "source": [
        "batch_size = 32"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ndUDse05Iu5D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b1a0b2e-3602-4e57-c3c7-c6eaccf8b4b1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 3040 images belonging to 9 classes.\n"
          ]
        }
      ],
      "source": [
        "train_generator = train_datagen.flow_from_directory(\n",
        "        train_data_dir,\n",
        "        target_size=(img_rows, img_cols),\n",
        "        batch_size=batch_size,\n",
        "        class_mode='categorical')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "skF6dZ6wIu8M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "84a93ce0-6268-4440-a6b7-f34c75f924c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 15151 images belonging to 9 classes.\n"
          ]
        }
      ],
      "source": [
        "validation_generator = validation_datagen.flow_from_directory(\n",
        "        validation_data_dir,\n",
        "        target_size=(img_rows, img_cols),\n",
        "        batch_size=batch_size,\n",
        "        class_mode='categorical')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lB4iJuSxIu_I"
      },
      "outputs": [],
      "source": [
        "from keras.optimizers import RMSprop\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "riyZ4xf3IvB2"
      },
      "outputs": [],
      "source": [
        "\n",
        "checkpoint = ModelCheckpoint(\"/root/face_vgg16.h5\",\n",
        "                             monitor=\"val_loss\",\n",
        "                             mode=\"min\",\n",
        "                             save_best_only = True,\n",
        "                             verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oxtvp9CBIvE_"
      },
      "outputs": [],
      "source": [
        "earlystop = EarlyStopping(monitor = 'val_loss',\n",
        "                          min_delta = 0,\n",
        "                          patience = 3,\n",
        "                          verbose = 1,\n",
        "                          restore_best_weights = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VCnDWP8lIvH6"
      },
      "outputs": [],
      "source": [
        "callbacks = [earlystop, checkpoint]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rAR4TkjlIvLF"
      },
      "outputs": [],
      "source": [
        "modelnew.compile(loss = 'categorical_crossentropy',\n",
        "              optimizer = RMSprop(learning_rate = 0.001),\n",
        "              metrics = ['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8oOuYI0qIvOM"
      },
      "outputs": [],
      "source": [
        "nb_train_samples = 300\n",
        "nb_validation_samples = 50"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SeH3BTLcIvRL"
      },
      "outputs": [],
      "source": [
        "epochs = 5\n",
        "batch_size = 32"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tGVpx3F3IvUa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d8fb34d8-34a8-4b9d-d353-3ab1d34c1bd0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "9/9 [==============================] - ETA: 0s - loss: 1.8447 - accuracy: 0.4167 \n",
            "Epoch 1: val_loss improved from inf to 4.02405, saving model to /root/face_vgg16.h5\n",
            "9/9 [==============================] - 253s 27s/step - loss: 1.8447 - accuracy: 0.4167 - val_loss: 4.0241 - val_accuracy: 0.0312\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/5\n",
            "9/9 [==============================] - ETA: 0s - loss: 1.5316 - accuracy: 0.4549 \n",
            "Epoch 2: val_loss improved from 4.02405 to 3.35139, saving model to /root/face_vgg16.h5\n",
            "9/9 [==============================] - 245s 26s/step - loss: 1.5316 - accuracy: 0.4549 - val_loss: 3.3514 - val_accuracy: 0.1562\n",
            "Epoch 3/5\n",
            "9/9 [==============================] - ETA: 0s - loss: 1.5032 - accuracy: 0.4757 \n",
            "Epoch 3: val_loss did not improve from 3.35139\n",
            "9/9 [==============================] - 238s 26s/step - loss: 1.5032 - accuracy: 0.4757 - val_loss: 4.2392 - val_accuracy: 0.0625\n",
            "Epoch 4/5\n",
            "9/9 [==============================] - ETA: 0s - loss: 1.5114 - accuracy: 0.4375 \n",
            "Epoch 4: val_loss did not improve from 3.35139\n",
            "9/9 [==============================] - 238s 26s/step - loss: 1.5114 - accuracy: 0.4375 - val_loss: 3.6042 - val_accuracy: 0.1875\n",
            "Epoch 5/5\n",
            "9/9 [==============================] - ETA: 0s - loss: 1.4406 - accuracy: 0.5174 Restoring model weights from the end of the best epoch: 2.\n",
            "\n",
            "Epoch 5: val_loss did not improve from 3.35139\n",
            "9/9 [==============================] - 280s 31s/step - loss: 1.4406 - accuracy: 0.5174 - val_loss: 3.4467 - val_accuracy: 0.0625\n",
            "Epoch 5: early stopping\n"
          ]
        }
      ],
      "source": [
        "history = modelnew.fit(\n",
        "    train_generator,\n",
        "    steps_per_epoch = nb_train_samples // batch_size,\n",
        "    epochs = epochs,\n",
        "    callbacks = callbacks,\n",
        "    validation_data = validation_generator,\n",
        "    validation_steps = nb_validation_samples // batch_size )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MEtZS9AGIvXQ"
      },
      "outputs": [],
      "source": [
        "modelnew.save(\"/root/face_vgg16.h5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HYFep4rjIvZ9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f6254a62-84ef-4eb6-bbfd-174f4daeae91"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.0625\n"
          ]
        }
      ],
      "source": [
        "final_accuracy=history.history[\"val_accuracy\"][-1]\n",
        "print(final_accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "piM7qPEOIvc4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f7059d8-a0de-4fc9-962a-eb069f2ba315"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 3040 images belonging to 9 classes.\n",
            "Class Names: ['Ford', 'Honday', 'Hyundai', 'Nissan', 'Renault', 'Suzuki', 'Tata', 'Toyota', 'Volkswagen']\n"
          ]
        }
      ],
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Define the directory where your dataset is located\n",
        "data_dir = '/content/drive/MyDrive/datasets/train'  # Replace with the path to your dataset\n",
        "\n",
        "# Create an ImageDataGenerator for the dataset\n",
        "datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "# Create a generator for the dataset\n",
        "generator = datagen.flow_from_directory(\n",
        "    data_dir,\n",
        "    target_size=(224, 224),  # Set the target image size\n",
        "    batch_size=32,           # Set the batch size\n",
        "    class_mode='categorical' # Set the class mode\n",
        ")\n",
        "\n",
        "# Get the class names from the generator\n",
        "class_names = list(generator.class_indices.keys())\n",
        "\n",
        "# Print the class names\n",
        "print(\"Class Names:\", class_names)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LqGx33yvIvff",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "499eb539-62b1-4a23-b297-abf012f96900"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 1s 736ms/step\n",
            "Predicted Class Name: Suzuki\n"
          ]
        }
      ],
      "source": [
        "from keras.preprocessing import image\n",
        "import numpy as np\n",
        "\n",
        "# Load the trained model\n",
        "from keras.models import load_model\n",
        "modelnew = load_model(\"/root/face_vgg16.h5\")\n",
        "\n",
        "# Define a function to predict the class of an image\n",
        "def predict_class(model, image_path):\n",
        "    img = image.load_img(image_path, target_size=(224, 224))\n",
        "    img = image.img_to_array(img)\n",
        "    img = np.expand_dims(img, axis=0)\n",
        "    img = img / 255.0  # Normalize the image data if needed\n",
        "\n",
        "    prediction = model.predict(img)\n",
        "    class_index = np.argmax(prediction, axis=1)\n",
        "    return class_index[0]\n",
        "\n",
        "image_path='/content/drive/MyDrive/datasets/test/1.png' #Replace with your test image filepath\n",
        "predicted_class = predict_class(modelnew, image_path)\n",
        "\n",
        "predicted_class_name = class_names[predicted_class]\n",
        "\n",
        "print(\"Predicted Class Name:\", predicted_class_name)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}